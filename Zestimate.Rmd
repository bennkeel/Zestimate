---
title: 'Regrestimate: Predicting Charlotte, NC Home Sale Prices'
author: "Ann (Zi'an) Zhang, Ben Keel"
date: "2022-10-14"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    css:
editor_options: 
  markdown: 
    wrap: 72
---
## Starting Point
<Describing the set up>


```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)


options(scipen=999)

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

library(tidycensus)
census_api_key("3c9540be1434ac4b38e6e55d60e8ee95909f2254", overwrite = TRUE)

```

<Describing the original data set>

```{r}

#original dataset
Charlotte.nhoods <- st_read(file.path("https://bennkeel.github.io/Zestimate/Area(1).geojson"))  %>%
  st_transform('ESRI:103501')

#
Charlotte <-
  st_read("https://raw.githubusercontent.com/mafichman/MUSA_508_Lab/main/Midterm/data/2022/studentData.geojson") %>%
  st_transform('ESRI:103501')

```

## Variables we Liked
<Gathering Data Paragraph>

```{r Variables We Liked}

Charlotte.Clean <-
  Charlotte %>%
  dplyr::select("pid", "nc_pin", "municipali", "yearbuilt", "heatedarea", "price", "storyheigh", "heatedfuel", "actype", "extwall", "foundation", "numfirepla", "fireplaces", "bldggrade", "fullbaths", "halfbaths", "bedrooms", "toPredict", "landusecod", "musaID") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(price <= 10000000)

Charlotte.Clean$fireplaces[is.na(Charlotte.Clean$fireplaces)] <- "FP0"

Charlotte.Clean <-
  st_join(Charlotte.Clean, Charlotte.nhoods, join=st_intersects)

```

Additonal Variables

```{r}

#Park location information
park <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/735a6bce6306442face38657b50fc7b7_10/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501') 
  
park.buffer <-
  park %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$park.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(park.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$park.Buffer[is.na(Charlotte.Clean$park.Buffer)] <- 0


#School location information
school <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/04f2ea0b58774ee7b2e525816cbbc0bb_1/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

school.buffer <-
  school %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit()

Charlotte.Clean$school.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(school.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$school.Buffer[is.na(Charlotte.Clean$school.Buffer)] <- 0


#Hospital Proximity
medical <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/320dbc7d1ef944f5bf7c5e21b018b678_4/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501')

med.buffer <-
  medical %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$med.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(med.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$med.Buffer[is.na(Charlotte.Clean$med.Buffer)] <- 0
  

#Food Access
grocery <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/93e54082dde0418d836a57f2fc12879f_7/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501') %>%
  dplyr::select("OBJECTID", "geometry")

shoppingmall <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/58487298f4ee455e84e236b5db43195d_11/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  dplyr::select("OBJECTID", "geometry")

shopping <-
  rbind(grocery, shoppingmall)

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(
      shop_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(shopping), k = 1))

#Health Access
pharmacies <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/dcfdb72bc9c045a0b0945da79a966841_3/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(pharm_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(pharmacies), k = 1))

#High-profile crime
homocide <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/b5b21fcd2ad24de9ba7a13093648f5e9_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501')

homicide.buffer <-
  homicide %>%
  dplyr::select(LATITUDE_PUBLIC, LONGITUDE_PUBLIC) %>%
  st_as_sf(coords = c("LONGITUDE_PUBLIC", "LATITUDE_PUBLIC"), crs = 4326) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$homi.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(homicide.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$homi.Buffer[is.na(Charlotte.Clean$homi.Buffer)] <- 0


#Low-profile Crime
incidents <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/d22200cd879248fcb2258e6840bd6726_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(inci_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(incidents), k = 1))


```

### Summary Statistics

<Paragraph>
<Data sorted by internal characteristics, local surroundings, spatial structure>

```{r Summary Statistics, warning = FALSE, message = FALSE}



```


### Correlation Matrix

<Paragraph>

```{r Correlation Matrix, warning = FALSE, message = FALSE}

Charlotte.Modelling <-
  Charlotte.Clean %>%
  filter(toPredict=="MODELLING") 

Charlotte.Challenge <-
  Charlotte.Clean %>%
  filter(toPredict=="CHALLENGE")

numericVars <- 
  select_if(st_drop_geometry(Charlotte.Modelling), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 

```

### Home Price Scatterplots

Present 4 home price correlation scatterplots that you think are of interest.

```{r Home Price Scatterplots, warning = FALSE, message = FALSE}



```

### Map of Home Prices and Indicators

<Paragraph>

```{r Map of Home Prices, warning = FALSE, message = FALSE}



```

### Map 1 


```{r Indicator Map 1, warning = FALSE, message = FALSE}



```

### Map 2 

Map 2 of our second most interesting predictor.

```{r Indicator Map 2, warning = FALSE, message = FALSE}



```

### Map 3 

Map 3 of our third most interesting predictor.

```{r Indicator Map 3, warning = FALSE, message = FALSE}



```

## Prediction Methods

<Paragraph>

```{r Prediction Methods, warning = FALSE, message = FALSE}




```

## Results

### Training Set Results

<Paragraph>

```{r Training Set, warning = FALSE, message = FALSE}

#Creating the partition between training and testing
inTrain <- createDataPartition(
              y = paste(Charlotte.Modelling$storyheigh, Charlotte.Modelling$actype, Charlotte.Modelling$aheatingty, Charlotte.Modelling$heatedfuel, Charlotte.Modelling$extwall, Charlotte.Modelling$foundation, Charlotte.Modelling$numfirepla, Charlotte.Modelling$fireplaces, Charlotte.Modelling$bldggrade, Charlotte.Modelling$fullbaths, Charlotte.Modelling$halfbaths, Charlotte.Modelling$bedrooms, Charlotte.Modelling$municipali, Charlotte.Modelling$landusecod), 
              p = .60, list = FALSE)
Charlotte.training <- Charlotte.Modelling[inTrain,] 
Charlotte.test <- Charlotte.Modelling[-inTrain,]  

#OLS Regression function
reg.training <- 
  lm(price ~ ., data = as.data.frame(Charlotte.training) %>% 
  dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel, actype, extwall, foundation, numfirepla, fireplaces, bldggrade, fullbaths, halfbaths, bedrooms, municipali, landusecod))

#Quick Summary (raw info)
reg.training%>%
  tidy()%>%
  kable(
    caption = "<center><strong>Regression Variables</strong></center>",
    escape= FALSE,
    format="html")%>%
  kable_styling(bootstrap_options = "striped")

###Does not have r-squared, etc at bottom.

```

### Test Set Measurements

<Paragraph>

```{r Test Set Results, warning = FALSE, message = FALSE}

Charlotte.test <-
  Charlotte.test %>%
  mutate(Regression = "Baseline Regression",
         Price.Predict = predict(reg.training, Charlotte.test),
         Price.Error = Price.Predict - price,
         Price.AbsError = abs(Price.Predict - price),
         Price.APE = (abs(Price.Predict - price)) / Price.Predict) %>%
  filter(price < 10000000) %>%
  na.omit(Charlotte.test)

#Kable table for summary of Mean Absolute Error and MAPE(%)

tracts_LimitSD.Summary %>%
  unite(year.TOD, year, TOD, sep = ": ", remove = T) %>%
  gather(Variable, Value, -year.TOD) %>%
  mutate(Value = round(Value, 2)) %>%
  spread(year.TOD, Value) %>%
  kable() %>%
  kable_styling() %>%
  footnote(general_title = "\n",
           general = credit)

  
st_drop_geometry(Charlotte.test)%>%
  dplyr::select(Price.AbsError, Price.APE)%>%
  summarize_at(vars(Price.AbsError, Price.APE), list(mean))%>%
  rename(MeanAbsoluteError = Price.AbsError,
         MAPE = Price.APE)%>%
  kable(
    caption = "<center><strong>Average Regression Errors by Value and Percent</strong></center>",
    escape= FALSE,
    format="html",
    row.names = FALSE,
    align="l")%>%
  kable_styling()

```

### Cross-Validation Tests & Results

<Paragraph>

```{r CV}

fitControl <- trainControl(method = "cv", number = 100)
set.seed(201)

reg.cv <- 
  train(price ~ ., data = st_drop_geometry(Charlotte.Clean) %>% 
                              dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel,
                                            actype, extwall, foundation, numfirepla, fireplaces,
                                            bldggrade, fullbaths, halfbaths, bedrooms, municipali,
                                            landusecod),
        method = "lm", trControl = fitControl, na.action = na.pass)

summary.cv <- reg.cv$resample[1:100,]

data.frame(Standard_Deviation = c(sd(summary.cv$MAE)), 
          Mean = c(mean(summary.cv$MAE)))%>%
    kable(
    caption = "<center><strong>Cross Validation: Mean Absolute Error of 100 Samples</strong></center>",
    escape= FALSE,
    format="html",
    row.names = FALSE, 
    align="l")%>%
  kable_styling()


```


#### K-fold Cross Validation

```{r CV Results, warning = FALSE, message = FALSE}

ggplot(summary.cv, aes(x=MAE)) +
  geom_histogram(color="blue", fill = "blue")+
  labs(title = "Distribution of MAE")+
  plotTheme()

```
#### Is the model generalizable to new data?

<Talk about it here>

### Predicted vs Observation Scatterplot

<Paragraph>

```{r Predicted vs Observation Scatterplot, warning = FALSE, message = FALSE}

summary.fitTest <- Charlotte.test%>%
  dplyr::select(price, Price.Predict)%>%
  rename(Observed_Price = price,
         Predicted_Price = Price.Predict)

ggscatter(summary.fitTest,
          x = "Observed_Price",
          y = "Predicted_Price") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  geom_abline(color = "orange", size = 2) +
  labs(title = "Predicted Prices as a Function of Observed Prices", 
       subtitle = "The orange line is a perfect prediction, \nthe green line is our prediction")+
  stat_cor(label.y = 4000000)


```

### Residuals Map

<Paragraph>

```{r Residuals Map, warning = FALSE, message = FALSE}

ggplot()+
  geom_sf(data=Charlotte.nhoods, fill="gray", color="black")+
  geom_sf(data=Charlotte.test, aes(color=q5(Price.AbsError)), size = 0.05)+
  scale_color_manual(values = palette5,
                     labels = qBr(Charlotte.test, "Price.AbsError"),
                     name= "Price Error\n(Quintile Breaks)")+
  labs(title="Residual Error of Price Estimation", subtitle = "Mecklenburg County, NC")+
  mapTheme()

```


#### Spatial Lag

```{r Moran's I Plot}

#Spatial lag of price
coords <- st_coordinates(Charlotte.Clean) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

Charlotte.Clean$lagPrice <- lag.listw(spatialWeights, Charlotte.Clean$price)

#Spatial lag of Errors
coords <- st_coordinates(Charlotte.test) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

Charlotte.test$lagPrice <- lag.listw(spatialWeights, Charlotte.test$Price.Error)

#Sales Price as Function of Spatial Lag of Price
lagClean <- ggscatter(Charlotte.Clean,
          x = "lagPrice",
          y = "price",
          xlab ="Spatial Lag of Price (Mean price 5 nearest neighbors)",
          ylab = "Observed Sale Price") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  labs(title = "Sales Price as Function of Spatial Lag of Price")

#Sales Price as Function of Spatial Lag of Error
lagError <- 
  ggscatter(Charlotte.test,
          x = "lagPrice",
          y = "price",
          xlab ="Spatial Lag of Errors (Mean error 5 nearest neighbors)",
          ylab = "Observed Sale Price") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  labs(title = "Sales Price as Function of Spatial Lag of Price") 

grid.arrange(lagClean, lagError)


```

#### Moran's I test

```{r Moran's I Plot}

moranTest <- moran.mc(Charlotte.test$Price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

### Predicted Map

<Paragraph>

```{r Predicted Map, warning = FALSE, message = FALSE}

predictAll <- lm(price ~ ., data = as.data.frame(Charlotte.Clean) %>% 
                   dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel, 
                                 actype, extwall, foundation, numfirepla, fireplaces, 
                                 bldggrade, fullbaths, halfbaths, bedrooms, municipali, 
                                 landusecod))

Charlotte.PredictAll <- Charlotte.Clean %>%
  mutate(Price.Predict = predict(predictAll, Charlotte.Clean))

ggplot()+
  geom_sf(data=st_union(Charlotte.nhoods), fill="gray", color="black")+
  geom_sf(data=Charlotte.PredictAll, aes(color=q5(Price.Predict)), size = 0.05)+
  scale_color_manual(values = palette5,
                     labels = qBr(Charlotte.test, "Price.Predict"),
                     name= "Prices ($)\n(Quintile Breaks)")+
  labs(title="Home Price Prediction", subtitle = "Mecklenburg County, NC")+
  mapTheme()



```

### MAPE by Neighborhood

<Provide a polished table of mean absolute error and MAPE for a single test set.>

<Paragraph> 

```{r MAPE by Neighborhood, warning = FALSE, message = FALSE}

#Prediction based on neighborhood (From Text)

left_join(
  st_drop_geometry(Charlotte.test) %>%
    group_by(npa) %>%
    summarize(meanPrice = mean(price, na.rm = T)),
  mutate(Charlotte.test, predict.fe = 
                        predict(lm(price ~ npa, data = Charlotte.test), 
                        Charlotte.test)) %>%
    st_drop_geometry %>%
    group_by(npa) %>%
      summarize(meanPrediction = mean(predict.fe))) %>%
      kable() %>% kable_styling()

#MAPE based on neighborhood

Charlotte.npaMAPE <- st_drop_geometry(Charlotte.test) %>%
    group_by(npa) %>%
    summarize(MAPE = mean(Price.APE, na.rm = T), 
              MeanPrice = mean(price, na.rm=T))

Charlotte.npaMAPE%>%
    kable(caption = "<strong>Mean Absolute Percent Error (MAPE) by Neighborhood Profile Areas     (NPA's)</strong>",
    escape= FALSE,
    format="html",
    align = "l") %>% 
    kable_styling(bootstrap_options = "striped")

```

#### Map - MAPE by neighborhood mean price

<Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.>

<Paragraph>

```{r MAPE map by neighborhood, warning = FALSE, message = FALSE}

g(Charlotte.test)

Charlotte.npaMAPEmap <- left_join(Charlotte.nhoods, Charlotte.npaMAPE, by="npa")

 ggplot()+
  geom_sf(data=Charlotte.npaMAPEmap, aes(fill=q5(MAPE)))+
  scale_fill_manual(values = palette5,
                     labels = qBr(Charlotte.npaMAPEmap, "MAPE"),
                     name= "MAPE\n(Quintile Breaks)")+
  labs(title="MAPE by Neighborhood", subtitle = "Mecklenburg County, NC")+
  mapTheme()

```


#### Scatterplot - MAPE by neighborhood mean price

<Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.>

<Paragraph>

```{r MAPE scatterplot, warning = FALSE, message = FALSE}

ggscatter(Charlotte.npaMAPE,
          x = "MeanPrice",
          y = "MAPE",
          xlab ="Average Price of Homes in the Neighborhood)",
          ylab = "Mean Absolute Percent Error (MAPE)") +
  labs(title = "Neighborhoods: MAPE as a function of mean price") 

```

### Split by Census Group

<Using tidycensus, split your study area into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?>

<Paragraph>

```{r Split by Census Group Map, warning = FALSE, message = FALSE}

#Majority Above 100% AMI for household of 2-3

tracts20 <- 
  get_acs(geography = "tract", 
          variables = c("B25010_001", "B19001_001",
                        "B19001_013", "B19001_014", 
                        "B19001_015", "B19001_016", 
                        "B19001_017"), 
          year=2020, state=37,
          county=119, geometry=TRUE, output = 'wide') %>% 
  st_transform('ESRI:103501') %>%
  rename(MeanHHSize = B25010_001E,
         totalHH = B19001_001E,
         HHIncome75to99 = B19001_013E,
         HHIncome100to124 = B19001_014E,
         HHIncome125to149 = B19001_015E,
         HHIncome150to199 = B19001_016E,
         HHIncome200up = B19001_017E
         ) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(HHAbove75 = HHIncome75to99 + HHIncome100to124 + HHIncome125to149 +
         HHIncome150to199 + HHIncome200up, 
         pctAbove75 = HHAbove75/totalHH,
         above75 = ifelse(pctAbove75 > .5, "High Income", "Low Income")) %>%
  dplyr::select(-HHIncome75to99, -HHIncome100to124, -HHIncome125to149,
                -HHIncome150to199, -HHIncome200up)

g(tracts20)

median(tracts20$MeanHHSize, na.rm=TRUE)

ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = above75)) +
  scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Level of tract") +
  labs(title = "Annual Income above $75k") +
  mapTheme() + theme(legend.position="bottom")

```
``` {r Census Group Analysis Chart}
#neighborhood regression model (we'll substitute with npa)
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                                 dplyr::select(Name, SalePrice, LivingArea, 
                                               Style, GROSS_AREA, NUM_FLOORS.cat,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE,crimes.Buffer))

#creating df with neighborhood adjustments
boston.test.nhood <-
  boston.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, boston.test),
         SalePrice.Error = SalePrice.Predict- SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict- SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict- SalePrice)) / SalePrice)%>%
  filter(SalePrice < 5000000)

#creating df just with essential comparison data
bothRegressions <- 
  rbind(
    dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))  

#spatial join of the essential comparison data with our tract data
st_join(bothRegressions, tracts20) %>% 
  group_by(Regression, above75) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

#calculating lag price error for each model
bothRegressions <- 
  rbind(
    dplyr::select(Charlotte.test, starts_with("price"), Regression, npa) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, Price.Error)),
    dplyr::select(Charlotte.test.nhoods, starts_with("price"), Regression, npa) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, Price.Error)))   

#table of the abs and ape errors
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -npa) %>%
  filter(Variable == "Price.AbsError" | Variable == "Price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

```{r Census Group Analysis Map}

#summarize by MAPE
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Name) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods) %>%
    st_sf() %>%
  #plot the graph
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      geom_sf(data = bothRegressions, colour = "black", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      mapTheme()

```

## Evaluating our Model

Is this an effective model? 
What were some of the more interesting variables?
How much of the variation in prices could you predict?
Describe the more important features? Describe the error in your predictions? 
According to your maps, could you account the spatial variation in prices?  
Where did the model predict particularly well? Poorly? Why do you think this might be?

### Accuracy


### Generalizability

<Paragraph>

## Conclusion