---
title: 'Regrestimate: Predicting Charlotte, NC Home Sale Prices with the Hedonic Model and OLS Regression'
author: "Ann (Zi'an) Zhang, Ben Keel"
date: "2022-10-14"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    css:
editor_options: 
  markdown: 
    wrap: 72
---
# Introduction

![Skyline of Charlotte, NC. Image by Carissa
Rogers](https://images.unsplash.com/photo-1539304180102-10f854c3585e?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1442&q=80)

## Zestier than Zillow?

This project uses the hedonic model and OLS regression for predicting home price in Mecklenburg County, NC, similar to the Zestimate tool one may use on Zillow. We use existing 46,183 cases to build the model -- 60% for training the model and 40% for testing the efficacy of the model. Once we produce a model with satisfactory efficacy, we then use this model to make prediction for 100 home units.

To generate a model with stronger predictive power, we incorporated more valid internal (structural) and external (neighborhoods, amenities, etc.) factors that may affect home price. The selection and engineering of those factors were executed with careful consideration of Mecklenburg County (including the City of Charlotte)'s local circumstances. After consulting local residents of Charlotte, we included factors like schools, crime, and area of parcel since schooling, safety, and having a yard are among their primary concerns when choosing houses.

# Data Source

The original data set contains basic information about the homes, including price, structural factors (e.g. number of stories), and ownership information. It consists of a modelling set of 46081 observations with existing price data and a challenge set of 100 observations, of which price is to be predicted using the model we generate.

In addition to the original data set provided to us, we selected data sets that are deemed valid factors for predicting housing price from the City of Charlotte Open Data Portal (<https://data.charlottenc.gov/>) and Charlotte/Mecklenburg Quality of Life Explorer, an online portal created by Mecklenburg County, the City of Charlotte, and UNC Charlotte(<https://mcmap.org/qol/#43/>).

**Note:** information about neighborhood in the county are taken from the Quality of Life Explorer. Mecklenburg County currently utilizes the system of "neighborhood profile area (NPA)," which is generated from the old "neighborhood statistic area (NSA)." Instead of neighborhood names, NPA is encoded in ID-numbers. There are 458 NPAs in the Mecklenburg County.

## Data Analysis

This project requires several R packages to be loaded, including but not limited to 'FNN' package for calculating the distance to the nearest shop/hospital/pharmacy ('K-nearest-neighbor') and 'stargazer' and 'kableExtra' for constructing tables.

```{r setup, warning = FALSE, message = FALSE,  results = 'hide'}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Load some libraries

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)

g <-glimpse

options(scipen=999)

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

library(tidycensus)
census_api_key("3c9540be1434ac4b38e6e55d60e8ee95909f2254", overwrite = TRUE)

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
Pink <- c("#ffffff","#f9f4f4","#f0e4e4", "#e7d3d3", "#dec3c3")
Blue <- c("#f8fbff", "#eaf4ff", "#d6eaff", "#add6ff", "#84c1ff")
Violet <- c("#ffffff", "#f7f7f7", "#dfe3ee", "#8b9dc3",  "#58668b")
Green <- c("#e8f4ea", "#e0f0e3", "#d2e7d6","#c8e1cc", "#b8d8be")
Purple <- c("#f3e0f7", "#e4c7f1", "#d1afe8", "#b998dd", "#9f82ce")
Teal <- c("#d1eeea", "#a8dbd9", "#85c4c9", "#68abb8", "#4f90a6")
P2Y <- c("#5E2BA3", "#88679E", "#B08CBD", "#D6B296", "#F2D412")
         
#original dataset
Charlotte.nhoods <- st_read(file.path("https://bennkeel.github.io/Zestimate/Area(1).geojson"))  %>%
  st_transform('ESRI:103501')%>%
  dplyr::select(-X2020)%>%
  rename(npa = id)

Charlotte <-
  st_read("https://raw.githubusercontent.com/mafichman/MUSA_508_Lab/main/Midterm/data/2022/studentData.geojson") %>%
  st_transform('ESRI:103501')

Charlotte.Clean <-
  Charlotte %>%
  dplyr::select("pid", "nc_pin", "municipali", "yearbuilt", "heatedarea", "price", "storyheigh", "heatedfuel", "actype", "extwall", "foundation", "numfirepla", "fireplaces", "bldggrade", "fullbaths", "halfbaths", "bedrooms", "toPredict", "landusecod", "shape_Area", "musaID") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(price <= 10000000)

Charlotte.Clean$fireplaces[is.na(Charlotte.Clean$fireplaces)] <- "FP0"

Charlotte.Clean <-
  st_join(Charlotte.Clean, Charlotte.nhoods, join=st_intersects)

```

## Additional Variables & Factor Engineering

After careful consideration of potential factors that impact the housing price, we extracted data of parks, schools, medical facilities, shops (including grocery stores and shopping malls), pharmacies, homicide (representing high-profile crime), and incidents (low-profile crime) from the open data portal. They become the independent variables in our model.

We then conducted factor engineering, namely, transforming existing data into a factor by establishing relationship between the independent variables with the dependent variable (home price) in two ways. Here, we use "k-nearest-neighbor" to calculated the distance from each home to the closest, 2nd closest, and 3rd closet shop/pharmacy, etc. and "aggregation within buffer" to calculated the number of shop/pharmacy, etc. within 1 mile range from each home.

```{r Additional Variables, warning = FALSE, message = FALSE, results = 'hide'}

#Park location information
park <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/735a6bce6306442face38657b50fc7b7_10/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501') 
  
park.buffer <-
  park %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$park.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(park.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$park.Buffer[is.na(Charlotte.Clean$park.Buffer)] <- 0

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(
      park_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(park), k = 1),
      
      park_nn2 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(park), k = 2), 
      
      park_nn3 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(park), k = 3))

#School location information
school <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/04f2ea0b58774ee7b2e525816cbbc0bb_1/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

school.buffer <-
  school %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit()

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(
      school_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(school), k = 1),
      
      school_nn2 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(school), k = 2), 
      
      school_nn3 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(school), k = 3))

Charlotte.Clean$school.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(school.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$school.Buffer[is.na(Charlotte.Clean$school.Buffer)] <- 0


#Hospital Proximity
medical <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/320dbc7d1ef944f5bf7c5e21b018b678_4/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501')

med.buffer <-
  medical %>%
  dplyr::select(geometry) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$med.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(med.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$med.Buffer[is.na(Charlotte.Clean$med.Buffer)] <- 0
  

#Food Access
grocery <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/93e54082dde0418d836a57f2fc12879f_7/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501') %>%
  dplyr::select("OBJECTID", "geometry")

shoppingmall <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/58487298f4ee455e84e236b5db43195d_11/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  dplyr::select("OBJECTID", "geometry")

shopping <-
  rbind(grocery, shoppingmall)

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(
      shop_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(shopping), k = 1),
      
      shop_nn2 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(shopping), k = 2), 
      
      shop_nn3 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(shopping), k = 3))

#Health Access
pharmacies <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/dcfdb72bc9c045a0b0945da79a966841_3/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(pharm_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(pharmacies), k = 1))

#High-profile crime
homicide <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/b5b21fcd2ad24de9ba7a13093648f5e9_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_transform('ESRI:103501')

homicide.buffer <-
  homicide %>%
  dplyr::select(LATITUDE_PUBLIC, LONGITUDE_PUBLIC) %>%
  st_as_sf(coords = c("LONGITUDE_PUBLIC", "LATITUDE_PUBLIC"), crs = 4326) %>%
  st_transform('ESRI:103501') %>%
    na.omit() 

Charlotte.Clean$homi.Buffer <- 
    Charlotte.Clean$geometry %>% 
    st_buffer(5280) %>% 
    aggregate(mutate(homicide.buffer, counter = 1),., sum) %>%
    pull(counter)

Charlotte.Clean$homi.Buffer[is.na(Charlotte.Clean$homi.Buffer)] <- 0


#Low-profile Crime
incidents <- 
  st_read("https://opendata.arcgis.com/api/v3/datasets/d22200cd879248fcb2258e6840bd6726_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:103501') %>%
  filter(!st_is_empty(.))

Charlotte.Clean <-
  Charlotte.Clean %>% 
    mutate(inci_nn1 = nn_function(st_coordinates(Charlotte.Clean), 
                              st_coordinates(incidents), k = 1))


```

### Separating Modeling vs. Predicting Sets

Since the original data set contains both the data used for making the
prediction model and the data to be predicted by the model, we separate
the original data set into "modelling" (for training model) and
"challenge" (to be predicted) sets.

```{r warning = FALSE, message = FALSE, result = 'hide'}

Charlotte.Modelling <-
  Charlotte.Clean %>%
  filter(toPredict=="MODELLING") 

Charlotte.Challenge <-
  Charlotte.Clean %>%
  filter(toPredict=="CHALLENGE")

```

## Summary Statistics

To better understand the variables that we considered as potentially
valid factors for predicting home prices, the following summary
statistic tables are created. The variables are split into the internal
characteristics, which are the inherent structural characteristics that
come with the house itself, and external factors, which includes local
surroundings, distance to amenities and civic services, and spatial
structures. Note that the tables only support summarizing numeric data,
so categorical variables such as 'fireplaces' (type of fireplaces) are
not included in the following tables.

```{r Summary Statistics, warning = FALSE, message = FALSE}

Charlotte.Summary <-
  Charlotte.Modelling %>%
  dplyr::select(-musaID) %>%
  st_drop_geometry %>%
  na.omit()

stargazer(Charlotte.Summary, type="text", omit = c("price", "yearbuilt", "heatedarea", "storyheigh", "heatedfuel", "actype", "extwall", "foundation", "numfirepla", "fireplaces", "bldggrade", "fullbaths", "halfbaths", "bedrooms"), title = "Table 1.2. External Characteristics Summary Statistics")

```

```{r, warning = FALSE, message = FALSE}
stargazer(Charlotte.Summary, type="text", omit = c("price", "yearbuilt", "heatedarea", "storyheigh", "heatedfuel", "actype", "extwall", "foundation", "numfirepla", "fireplaces", "bldggrade", "fullbaths", "halfbaths", "bedrooms"), title = "External Characteristics")
```

### Correlation Matrix

```{r Correlation Matrix, warning = FALSE, message = FALSE}

numericVars <- 
  select_if(st_drop_geometry(Charlotte.Modelling), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#B0E0E6", "white", "#B272A6"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation Across Numeric Variables", caption = "Fig. 2") 

```

This correlation matrix (Fig. 2) shows correlation across numeric
variables, including both internal structural and external neighborhood
characteristics. Higher correlation between any of the two variables
(other than price) suggest the possibility of col-linearity. As a
result, col-linear variables should be reduced to one that encapsulate
the effect of this factor. For example, since "shop_nn1," "shop_nn2,"
and "shop_nn3" (distance to the 1st/2nd/3rd closest shop) are all
col-linear with each other, we chose to only incorporate "shop_nn1"
(distance to the closest shop) into our regression model for home price
prediction and discard the other two.

## Home Price Scatter Plots

We selected four variables -- distances to the nearby homicide (Fig.
3.1), incident (Fig. 3.2), shops (Fig. 3.3), and schools (Fig. 3.4) to
look at their relationship with home prices via scatter plots. We chose
to look at the nearest 5 homicide and incidents and 3 shops/schools
because we assume residents wouldn't care too much about the 4th or 5th
closest shops/schools -- they would just go to closer ones.

```{r Home Price Scatterplots 1, warning = FALSE, message = FALSE}

#homicide
Charlotte.Modelling %>%  
  st_drop_geometry() %>%
  filter(price <= 1000000) %>%
  dplyr::select(price, starts_with("homi")) %>%
  gather(Variable, Value, -price) %>%
  ggplot(aes(Value, price)) +
      geom_point(size = .1, color = "light grey") +
      geom_smooth(method = "lm", se=F, color = "black") +
      facet_wrap(~Variable, nrow = 1, scales = "free") +
      labs(title = "Home Price & Distance to the (1st/2nd/3rd) Nearest Homicide", subtitle = "Mecklenburg County, NC", caption = "Fig. 3.1") +
      xlab("Distance (in ft.)") +
      ylab("Home Price") +
  theme(strip.text.x = element_text(size = 3)) +
  plotTheme()


```

```{r warning = FALSE, message = FALSE, fig.width=6}
#inci_nn1


Charlotte.Modelling %>%  
  st_drop_geometry() %>%
  filter(price <= 1000000) %>%
  dplyr::select(price, starts_with("inci_")) %>%
  gather(Variable, Value, -price) %>%
  ggplot(aes(Value, price)) +
      geom_point(size = .1, color = "#B0C4DE") +
      geom_smooth(method = "lm", se=F, color = "black") +
      facet_wrap(~Variable, nrow = 1, scales = "free") +
      labs(title = "Home Price & Distance to the (1st/2nd/3rd) Nearest Incident", subtitle = "Mecklenburg County, NC", caption = "Fig. 3.2") +
      xlab("Distance (in ft.)") +
      ylab("Home Price") +
  theme(strip.text.x = element_text(size = 3)) +
  plotTheme()

```

From the two scatter plots above, we can see a clear relationship that
the further a home is from a homicide and/or incident, the higher its
price may get. This aligns with the common understanding of people
paying more for homes in 'safer areas' (with less incidents and
homicide).

```{r warning = FALSE, message = FALSE, fig.width=6}

#shop_nn

Charlotte.Modelling %>%  
  st_drop_geometry() %>%
  filter(price <= 1000000) %>%
  dplyr::select(price, starts_with("shop_")) %>%
  gather(Variable, Value, -price) %>%
  ggplot(aes(Value, price)) +
      geom_point(size = .1, color = "#D8BFD8") +
      geom_smooth(method = "lm", se=F, color = "dark grey") +
      facet_wrap(~Variable, nrow = 1, scales = "free") +
      labs(title = "Home Price & Distance to the (1st/2nd/3rd) Nearest Shop", subtitle = "Mecklenburg County, NC", caption = "Fig. 3.3") +
      xlab("Distance (in ft.)") +
      ylab("Home Price") +
  theme(strip.text.x = element_text(size = 3)) +
  plotTheme()
 
```

```{r  warning = FALSE, message = FALSE, fig.width=6}

#school_nn
Charlotte.Modelling %>%  
  st_drop_geometry() %>%
  filter(price <= 1000000) %>%
  dplyr::select(price, starts_with("school_")) %>%
  gather(Variable, Value, -price) %>%
  ggplot(aes(Value, price)) +
      geom_point(size = .1, color = "#B4D7BF") +
      geom_smooth(method = "lm", se=F, color = "dark grey") +
      facet_wrap(~Variable, nrow = 1, scales = "free") +
      labs(title = "Home Price & Distance to the (1st/2nd/3rd) School", subtitle = "Mecklenburg County, NC", caption = "Fig. 3.4") +
      xlab("Distance (in ft.)") +
      ylab("Home Price") +
  theme(strip.text.x = element_text(size = 3)) +
  plotTheme()

```

Fig. 3.3 and Fig. 3.4, on the other hand, provides interesting insights
-- the further homes are from closest shops or schools, the higher their
price may get. This contradicts the expectations that convenience (to
shops and schools) comes at a price. This may be cause by a possible
confounding factor, such as closer to shops and schools might mean it's
a more densely populated area, although it remains uncertain.

### Park Buffer - Interesting Observation

```{r}

Charlotte.Modelling %>%  
  st_drop_geometry() %>%
  filter(price <= 1000000) %>%
  dplyr::select(price, starts_with("park_")) %>%
  gather(Variable, Value, -price) %>%
  ggplot(aes(Value, price)) +
      geom_point(size = .1, color = "light grey") +
      geom_smooth(method = "lm", se=F, color = "black") +
      facet_wrap(~Variable, nrow = 1, scales = "free") +
      labs(title = "Home Price & Distance to Closest Parks", subtitle = "Mecklenburg County, NC", caption = "Fig. 5.4") +
      xlab("Distance (in ft.)") +
      ylab("Home Price") +
  theme(strip.text.x = element_text(size = 3)) +
  plotTheme()

```

Another interesting observation is plotted in Fig. 5.4. If we look at
the relationship between home prices and their distance to the closest
parks, we see variation across the closest, 2nd closest, and 3rd closest
parks. The scatter plot suggest that the further the 'closest' park
lays, the home price goes down. However, the 2nd closest park doesn't
have a noticeable effect on home price, and the 3rd closest have the
opposite effect --- the further the 3rd closest park is, the home price
goes up.

This phenomenon may be caused the fact that if a home is located closely
to multiple parks, it may lay in a rather rural / remote area that are
too far from civic services, making it inconvenient and unappealing.

## Map of Home Prices and 3 Factors

We first mapped the home prices (Fig 5.1) to see the distribution of
home prices in existing 'Modelling' data set. Then, we mapped three
factors of interest - school buffers (which indicate the number of
schools within 1 mile buffer from each home) (Fig. 5.2), distance to
nearest shop (Fig. 5.3), and land use codes (Fig 5.4).

By comparing the latter 3 maps with the home price map, we can see that
homes are more expensive if they have more schools within 1 mile range,
especially concentrated in the southern area; home price is less
associated with distance to the nearest shops, since they are more
spread across the county. The land use code map brings us insight of the
distribution of different types of homes. From the map we can see that
the majority are single-family residents, with some categorized as near
"waterfront" or "rural acreage."

```{r Map of Home Prices, warning = FALSE, message = FALSE}

ggplot() + 
  geom_sf(data = st_union(Charlotte.nhoods), fill = "#000000") +
  geom_sf(data = Charlotte.Modelling, aes(color = q5(price)), show.legend = "point", size = .1) +
    scale_colour_manual(values = Blue, labels=qBr(Charlotte.Modelling,"price"), name="Home Price") +
    labs(title = "Home Price (Dependent Variable)", subtitle = "Mecklenburg County, NC") +
                          mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

### Map 1 School Buffer

```{r Indicator Map 1, warning = FALSE, message = FALSE}

ggplot() + 
  geom_sf(data = st_union(Charlotte.nhoods), fill = "#000000") +
  geom_sf(data = Charlotte.Modelling, aes(color = q5(school.Buffer)), show.legend = "point", size = .2) +
    scale_colour_manual(values = Pink, labels=qBr(Charlotte.Modelling,"school.Buffer"), name="School Buffer") +
    labs(title = "Number of Schools Within 1 Mile Buffer", subtitle = "Mecklenburg County, NC", aes(color= "pink"), caption = "Fig. 5.1") +
                          mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

### Map 2 Shop nn1

Map 2 of our second most interesting predictor.

```{r Indicator Map 2, warning = FALSE, message = FALSE}

ggplot() + 
  geom_sf(data = st_union(Charlotte.nhoods), fill = "black", color="black", size=0.5) +
  geom_sf(data = Charlotte.Modelling, aes(color = q5(shop_nn1)), show.legend = "point", size = .1) +
    scale_colour_manual(values = Green, labels=qBr(Charlotte.Modelling,"shop_nn1"), name="Home Price") +
    labs(title = "Distance to the Nearest Shop=", subtitle = "Mecklenburg County, NC", caption = "Fig. 5.2") +
                          mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

### Map 3 land use code

Map 3 of our third most interesting predictor.

```{r Indicator Map 3, warning = FALSE, message = FALSE}

Charlotte.landuse <- Charlotte.Modelling%>%
  dplyr::select(landusecod, geometry)%>%
  dplyr::filter(grepl("R", landusecod))

Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R100"] <- "SF_Res"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R111"] <- "SF_Res.Common" 
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R113"] <- "SF_Res.River"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R120"] <- "SF_Res.Rural"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R122"] <- "SF_Res.Waterfront"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R123"] <- "SF_Res.Golf" 
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R124"] <- "SF_Res.WaterView"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R200"] <- "Mobile_Homes"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R300"] <- "Condo"
Charlotte.landuse$landusecod[Charlotte.landuse$landusecod == "R309"] <- "TownHouse_SF_Res"

ggplot() + 
  geom_sf(data = st_union(Charlotte.nhoods), fill = "#203354") +
  geom_sf(data = Charlotte.landuse, aes(color=landusecod), show.legend = "point", size = .1) +
    labs(title = "Land Use Code (Residential)", subtitle = "Mecklenburg County, NC", caption = "Fig. 5.3") + 
  mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

# Prediction Methods

Having examined the available data, we moved on to our first draft of predictions. 

We started the process by splitting the supplied data set into two groups: a larger one to supply information to our regression model, and smaller one to test whether that model is working. Then we worked through multiple iterations of the model using the interesting variables we'd gathered, plugging them into the regression function as predictors of sale `price`. 

A couple rounds of iteration revealed what was contributing to the model and what was just extra. We tried to avoid too many variables that would account for the same reasons that would affect a home's price, and coefficients of each variable helped us determine what was most helpful. Those predictors together explained a certain amount of the model, represented the the r-squared summary value shown below. 

While explaining 100% of the variance in a model would be powerful, it's very unlikely with our current tools. As such, our goal was explaining 65-70% of the variance, and we kept iterating until that point.

```{r Prediction Methods, warning = FALSE, message = FALSE}

inTrain <- createDataPartition(
              y = paste(Charlotte.Modelling$storyheigh, Charlotte.Modelling$actype, Charlotte.Modelling$aheatingty, Charlotte.Modelling$heatedfuel, Charlotte.Modelling$extwall, Charlotte.Modelling$foundation, Charlotte.Modelling$numfirepla, Charlotte.Modelling$fireplaces, Charlotte.Modelling$bldggrade, Charlotte.Modelling$fullbaths, Charlotte.Modelling$halfbaths, Charlotte.Modelling$bedrooms, Charlotte.Modelling$municipali, Charlotte.Modelling$landusecod, Charlotte.Modelling$npa), 
              p = .60, list = FALSE)
Charlotte.training <- Charlotte.Modelling[inTrain,] 
Charlotte.test <- Charlotte.Modelling[-inTrain,]   

```

# Results

## Training Set Results

Reviewing the table below: R2, or R-squared, is that percentage we were seeking, and we met our goal of ~70% of price variance captured within this model. Within the individual variables in the second table, many were influential in the prediction, signaled by their very low p-values. Some proved influential as expected from their correlation tests, like inside heated area, while others like building grade were showing their influence for the first time in this model.


To explain the long list, we were using a number of categorical variables like land use code and building heights, and each category will be listed in the results. 

```{r Training Set, warning = FALSE, message = FALSE}

reg.training <- 
  lm(price ~ ., data = as.data.frame(Charlotte.training) %>% 
  dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel, actype, extwall, foundation, numfirepla, fireplaces, bldggrade, fullbaths, halfbaths, bedrooms, pharm_nn1, inci_nn1, homi.Buffer, school.Buffer, med.Buffer, municipali, landusecod, park.Buffer, shop_nn1, shape_Area))

#Quick Summary (raw info)
stargazer(reg.training, type="text", title="Regression Results", keep="heatedarea, numfirepla")

reg.training%>%
  tidy()%>%
  kable(
    caption = "<strong>Additional Regression Variables</strong>",
    escape= FALSE,
    format="html")%>%
  kable_styling(bootstrap_options = "striped")%>%
  scroll_box(width = "800px", height = "200px")

```

## Test Set Measurements

Some more tangible terms are shown in this table below, calculated by applying the model to the smaller test set of homes we set aside earlier. The Mean Absolute Error, illustrates that our predictions were an average of $74,034.87 different than the actual price of a home. Proportionately, that's 21% off-base in this market, as shown by the Mean Absolute Percent Error (MAPE).

```{r Test Set Results, warning = FALSE, message = FALSE}

Charlotte.test <-
  Charlotte.test %>%
  mutate(Regression = "Baseline Regression",
         Price.Predict = predict(reg.training, Charlotte.test),
         Price.Error = Price.Predict - price,
         Price.AbsError = abs(Price.Predict - price),
         Price.APE = (abs(Price.Predict - price)) / Price.Predict) %>%
  filter(price < 10000000) %>%
  na.omit(Charlotte.test)

#Kable table for summary of Mean Absolute Error and MAPE(%)

st_drop_geometry(Charlotte.test)%>%
  dplyr::select(Price.AbsError, Price.APE)%>%
  summarize_at(vars(Price.AbsError, Price.APE), list(mean))%>%
  rename(MeanAbsoluteError = Price.AbsError,
         MAPE = Price.APE)%>%
  kable(
    caption = "<strong>Average Regression Errors by Value and Percent</strong>",
    escape= FALSE,
    format="html",
    row.names = FALSE,
    align="l")%>%
  kable_styling()

```

# Cross-Validation Tests

One test set is good, but more validation of our result can help us figure out where we could improve our model.

## K-fold Cross Validation

We start this process through splitting up the data into 100 test sets and using the rest of the data to predict their price. We can then measure what errors we have and average them to get another Mean Absolute Error, which fortunately was a lower $67,655.36 off from actual prices.

```{r CV, warning = FALSE, message = FALSE}
#k-fold number selection
fitControl <- trainControl(method = "cv", number = 100)
set.seed(201)



reg.cv <- 
  train(price ~ ., data = st_drop_geometry(Charlotte.test) %>% 
                              dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel,
                                            actype, extwall, foundation, numfirepla, fireplaces,
                                            bldggrade, fullbaths, halfbaths, bedrooms, municipali,
                                            landusecod),
        method = "lm", trControl = fitControl, na.action = na.pass)

summary.cv <- reg.cv$resample[1:100,]

data.frame(Mean = c(mean(summary.cv$MAE)), 
           Standard_Deviation = c(sd(summary.cv$MAE)))%>%
    kable(
    caption = "<strong>Cross Validation: Mean Absolute Error of 100 Samples</strong>",
    escape= FALSE,
    format="html",
    row.names = FALSE, 
    align="l")%>%
  kable_styling()


```

Most of the predictions were between \$50,000 and \$75,000 off from actual prices.

```{r CV Results, warning = FALSE, message = FALSE}

ggplot(summary.cv, aes(x=MAE)) +
  geom_histogram(color="blue", fill = "blue")+
  labs(title = "Distribution of MAE")+
  plotTheme()

```

## Plotting Predictions vs Observations 

This plot is another way to look at our errors. It shows that we're under-predicting many home prices, as our green line of predicted fit is fairly under the orange perfect fit line.

```{r Predicted vs Observation Scatterplot, warning = FALSE, message = FALSE}

summary.fitTest <- Charlotte.test%>%
  dplyr::select(price, Price.Predict)%>%
  rename(Observed_Price = price,
         Predicted_Price = Price.Predict)

ggscatter(summary.fitTest,
          x = "Observed_Price",
          y = "Predicted_Price") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  geom_abline(color = "orange", size = 2) +
  labs(title = "Predicted Prices as a Function of Observed Prices", 
       subtitle = "The orange line is a perfect prediction, \nthe green line is our prediction")+
  stat_cor(label.y = 4000000)

```

## Residuals Map

We can also examine if errors are occurring in the same areas by mapping the homes and coloring their location with how off our model was. Visually checking can reveal errors quickly, but it requires knowledge of the areas or more thorough investigation.

```{r Residuals Map, warning = FALSE, message = FALSE}

ggplot()+
  geom_sf(data=Charlotte.nhoods, fill="white", color="gray")+
  geom_sf(data=Charlotte.test, aes(color=q5(Price.Error)), size = 0.4)+
  scale_color_manual(values = P2Y,
                     labels = qBr(Charlotte.test, "Price.Error"),
                     name= "Price Error\n(Quintile Breaks)")+
  labs(title="Residual Error of Price Estimation", subtitle = "Mecklenburg County, NC")+
  mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

## Spatial Lag

We know that homes near each other are probably within a similar range of values (location*3, right?). The first plot confirms that as a price of a home increases, often the price of its closest 5 neighbors increases as well. 

```{r SpatialLagPlot, warning = FALSE, message = FALSE}

#Spatial lag of price
coords <- st_coordinates(Charlotte.Clean) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

Charlotte.Clean$lagPrice <- lag.listw(spatialWeights, Charlotte.Clean$price)

#Spatial lag of Errors
coords.test <- st_coordinates(Charlotte.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

Charlotte.test$lagPrice.test <- lag.listw(spatialWeights.test, Charlotte.test$Price.Error)

#Sales Price as Function of Spatial Lag of Price
ggscatter(Charlotte.Clean,
          x = "lagPrice",
          y = "price",
          xlab ="Spatial Lag of Price (Mean price 5 nearest neighbors)",
          ylab = "Observed Sale Price",
          color = "light green") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  labs(title = "Sales Price as Function of Spatial Lag of Price")+
  stat_cor(label.y = 8000000)

```

Our errors may cluster as well, so we can do the same test of the five-nearest neighbors, but see how off our model was. As the previous scatterplot illsutrated, our model seems to be underpredicting, and as a home's price increases, our errors slightly tend to decrease. There's an obvious group outside the million-dollar error mark which may be affecting the trend line of this plot.

```{r SpatialLagPlotError, warning = FALSE, message = FALSE}
#Sales Price as Function of Spatial Lag of Error
ggscatter(Charlotte.test,
          x = "lagPrice.test",
          y = "price",
          xlab ="Spatial Lag of Errors (Mean error 5 nearest neighbors)",
          ylab = "Observed Sale Price", 
          color = "light green") +
  geom_smooth(color = "green", size = 2, method = "lm")+
  labs(title = "Sales Price as Function of Spatial Lag of Price")+
  stat_cor(label.y = 6000000)


```

## Moran's I test

Another way of checking for spatial clustering or dispersion is by comparing the location of our errors to a collection of those errors with randomized locations -- like comparing a stacked deck of cards to 999 other thoroughly-shuffled decks. Evidenced by the plot below of those 999 random samples (gray), our actual collection of errors (orange) are more clustered (closer to 1) than the rest of the shuffled sets of errors. 

```{r Morans, warning = FALSE, message = FALSE}

moranTest <- moran.mc(Charlotte.test$Price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()+
  guides(colour = guide_legend(override.aes = list(size=5)))

```

## Predicted Map

Acknowledging our limited tools and the remaining errors, the model can at least predict the broad trends home prices in the Charlotte, NC area. This map of our price predictions follows the "wedge and crescent" shapes that many Charlotte residents use to describe the area's neighborhood, with more expensive homes clustering in the a wedge shape at the bottom-middle of Mecklenburg County.

```{r Predicted Map, warning = FALSE, message = FALSE}

predictAll <- lm(price ~ ., data = as.data.frame(Charlotte.Clean) %>% 
                   dplyr::select(price, yearbuilt, heatedarea, storyheigh, heatedfuel, 
                                 actype, extwall, foundation, numfirepla, fireplaces, 
                                 bldggrade, fullbaths, halfbaths, bedrooms, municipali, 
                                 landusecod))

Charlotte.PredictAll <- Charlotte.Clean %>%
  mutate(Price.Predict = predict(reg.training, Charlotte.Clean))

ggplot()+
  geom_sf(data=st_union(Charlotte.nhoods), fill="gray", color="black")+
  geom_sf(data=Charlotte.PredictAll, aes(color=q5(Price.Predict)), size = 0.05)+
  scale_color_manual(values = palette5,
                     labels = qBr(Charlotte.test, "Price.Predict"),
                     name= "Prices ($)\n(Quintile Breaks)")+
  labs(title="Home Price Prediction", subtitle = "Mecklenburg County, NC")+
  mapTheme()+
  guides(colour = guide_legend(override.aes = list(size=3)))

```

## Neighborhood Effects

Considering the clustering of errors that our previous methods concluded, one direct method to address them is to include neighborhood as predictor in our model. Combining the NPA neighborhood data from Mecklenburg County's Quality of Life tool with our current data, shown in these tables, shows how wide our predictions and errors varied between neighborhoods.

```{r MAPE by Neighborhood, warning = FALSE, message = FALSE, results='hide'}

#Prediction based on neighborhood (From Text)

left_join(
  st_drop_geometry(Charlotte.test) %>%
    group_by(npa) %>%
    summarize(meanPrice = mean(price, na.rm = T)),
  mutate(Charlotte.test, predict.fe = 
                        predict(lm(price ~ npa, data = Charlotte.test), 
                        Charlotte.test)) %>%
    st_drop_geometry %>%
    group_by(npa) %>%
      summarize(meanPrediction = mean(predict.fe))) %>%
      kable() %>% kable_styling()%>%
      scroll_box(width = "800px", height = "200px")

#MAPE based on neighborhood

Charlotte.npaMAPE <- st_drop_geometry(Charlotte.test) %>%
    group_by(npa) %>%
    summarize(MAPE = mean(Price.APE, na.rm = T), 
              MeanPrice = mean(price, na.rm=T))

Charlotte.npaMAPE%>%
    kable(caption = "<strong>Mean Absolute Percent Error (MAPE) by Neighborhood Profile Areas     (NPA's)</strong>",
    escape= FALSE,
    format="html",
    align = "l") %>% 
    kable_styling(bootstrap_options = "striped")%>%
    scroll_box(width = "800px", height = "200px")

```

Being a spatial issue, mapping the errors by NPA illustrates some decent clustering around the map. Lack of data prevents all the NPA's from being shown here. 

```{r MAPE map by neighborhood, warning = FALSE, message = FALSE}

Charlotte.npaMAPEmap <- left_join(Charlotte.nhoods, Charlotte.npaMAPE, by="npa")%>%
  mutate(MAPEpct = MAPE * 100)


 ggplot()+
  geom_sf(data=Charlotte.npaMAPEmap, aes(fill=q5(MAPEpct)))+
  scale_fill_manual(values = Blue,
                     labels = qBr(Charlotte.npaMAPEmap, "MAPEpct"),
                     name= "MAPE\n(Quintile Breaks)")+
  geom_smooth(color = "green", size = 2, method = "lm")+
  labs(title="MAPE by Neighborhood", subtitle = "Mecklenburg County, NC")+
  mapTheme()

```

## Scatterplot - MAPE by neighborhood mean price

This scatterplot summarizes and shows the extremes of our mean absolute percent error per neighborhood. Again it shows how as home prices increase, our model will get slightly more accurate (disregarding a couple outliers). 

```{r MAPE scatterplot, warning = FALSE, message = FALSE}

ggscatter(Charlotte.npaMAPE,
          x = "MeanPrice",
          y = "MAPE",
          xlab ="Average Price of Homes in the Neighborhood)",
          ylab = "Mean Absolute Percent Error (MAPE)") +
  labs(title = "Neighborhoods: MAPE as a function of mean price")+
  stat_cor(label.y = 3)+      
  geom_smooth(method = "lm", se=F, color = "orange", size = 2)

```

## Accounting for Neighborhood in the Model

Each of these perspectives has proven that we should account for a neighborhood baseline in our model. To do so, we made a new training and testing set of home data, this time with NPA ID's added to each home's entry.

```{r Neighborhood Model, warning = FALSE, message = FALSE}

inTrain.nhood <- createDataPartition(
              y = paste(Charlotte.Modelling$storyheigh, Charlotte.Modelling$actype, Charlotte.Modelling$aheatingty, Charlotte.Modelling$heatedfuel, Charlotte.Modelling$extwall, Charlotte.Modelling$foundation, Charlotte.Modelling$numfirepla, Charlotte.Modelling$fireplaces, Charlotte.Modelling$bldggrade, Charlotte.Modelling$fullbaths, Charlotte.Modelling$halfbaths, Charlotte.Modelling$bedrooms, Charlotte.Modelling$municipali, Charlotte.Modelling$landusecod, Charlotte.Modelling$npa), 
              p = .60, list = FALSE)
Charlotte.training.nhood <- Charlotte.Modelling[inTrain.nhood,] 
Charlotte.test.nhood <- Charlotte.Modelling[-inTrain.nhood,]  


reg.nhood <- lm(price ~ ., data = as.data.frame(Charlotte.training.nhood) %>% 
                                 dplyr::select(price, yearbuilt, heatedarea, 
                                               storyheigh, heatedfuel, actype, extwall, 
                                               foundation, numfirepla, fireplaces, 
                                               bldggrade, fullbaths, halfbaths, bedrooms, 
                                               pharm_nn1, inci_nn1, 
                                             homi.Buffer, school.Buffer, med.Buffer, municipali, landusecod, shape_Area, npa))

Charlotte.test.nhoods <-
  Charlotte.test.nhood %>%
  mutate(Regression = "Neighborhood Effects",
         Price.Predict = predict(reg.nhood, Charlotte.test.nhood),
         Price.Error = Price.Predict - price,
         Price.AbsError = abs(Price.Predict - price),
         Price.APE = (abs(Price.Predict - price)) / Price.Predict) %>%
  filter(price < 10000000)%>%
  na.omit(Charlotte.test.nhoods)

```

## Comparison to Old Model

To quickly summarize how this new NPA factor affected the predictions, let's compare its error margins to the old model. We see here that accounting for NPA made the model around $8000 or 2.5% more accurate.

```{r Spatial Lag Each Model,warning = FALSE, message = FALSE}
#calculating lag price error for each model
bothRegressions <- 
  rbind(
    dplyr::select(Charlotte.test, starts_with("price"), Regression, npa) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, Price.Error)),
    dplyr::select(Charlotte.test.nhoods, starts_with("price"), Regression, npa) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, Price.Error)))

st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -npa) %>%
  filter(Variable == "Price.AbsError" | Variable == "Price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()%>%
    kable_styling()
```

## Generalizability via Census Groups

As a final evaluation, we checked how our model does within areas of different characteristics. Since our model originally had a greater magnitude of errors when it was looking at homes with lower values, checking how it performs relative to a neighborhood's income was a relevant test. 

So What was our parameter for a high-income or low-income area? According to the HouseCharlotte program's documents from the [Town of Davidson, NC site] (https://www.townofdavidson.org/DocumentCenter/View/10703/HC-AMI-Affordability-Income-Limits-July-1-2020), the area's median annual income in Charlotte was \$60K for a household of 2 and \$75k for a household of 3 in 2020. Census data, pulled via the tidycensus package, revealed that the median household size is 2.6 people. A household making above \$75k annually may not believe itself to be high-income, but it can fairly be considered in the upper half and $75k is a convenient census division line, so we moved forward with this dividing line. 

```{r Split by Census Group Map, warning = FALSE, message = FALSE, results = 'hide'}

#Majority Above 100% AMI for household of 2-3

tracts20 <- 
  get_acs(geography = "tract", 
          variables = c("B25010_001", "B19001_001",
                        "B19001_013", "B19001_014", 
                        "B19001_015", "B19001_016", 
                        "B19001_017"), 
          year=2020, state=37,
          county=119, geometry=TRUE, output = 'wide') %>% 
  st_transform('ESRI:103501') %>%
  rename(MeanHHSize = B25010_001E,
         totalHH = B19001_001E,
         HHIncome75to99 = B19001_013E,
         HHIncome100to124 = B19001_014E,
         HHIncome125to149 = B19001_015E,
         HHIncome150to199 = B19001_016E,
         HHIncome200up = B19001_017E
         ) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(HHAbove75 = HHIncome75to99 + HHIncome100to124 + HHIncome125to149 +
         HHIncome150to199 + HHIncome200up, 
         pctAbove75 = HHAbove75/totalHH,
         above75 = ifelse(pctAbove75 > .5, "Above", "Below")) %>%
  dplyr::select(-HHIncome75to99, -HHIncome100to124, -HHIncome125to149,
                -HHIncome150to199, -HHIncome200up)

median(tracts20$MeanHHSize, na.rm=TRUE) #2.6 Median Household Size in Charlotte

```
The census tracts shown below follow the same "wedge and crescent" pattern.

```{r MAPE with , warning = FALSE, message = FALSE}

ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = above75)) +
  scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Per Tract") +
  labs(title = "Neighborhoods w/ Income above $75k", subtitle = "Mecklenburg County, Census Tracts") +
  mapTheme()

```

### Combining Tracts + Neighborhoods

Once we overlap the tracts with NPA's and assign "Above" or "Below" $75k income to each NPA, we review our mean absolute percent error again for each with both the baseline model and the neighborhood model. Evidently, accounting for neighborhood helped mostly in the areas making below \$75K annually, which made it more generalizable.

```{r Census Group Analysis Chart, warning = FALSE, message = FALSE}

#spatial join of the essential comparison data with our tract data
st_join(bothRegressions, tracts20) %>% 
  group_by(Regression, above75) %>%
  summarize(mean.MAPE = scales::percent(mean(Price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(above75, mean.MAPE) %>%
  rename(Above_75K = Above,
         Below_75K = Below)%>%
  kable(caption = "<strong>Test set MAPE within Neighborhood Income Context</strong>")%>%
  kable_styling()

```

On this map set, We can see the areas along a middle latitude shift to a lighter value. While still very imperfect, our model was reinforced with this change and is better-equipped to handle home price prediction in areas with different ranges of household income.

```{r Census Group Analysis Map, warning = FALSE, message = FALSE}

#summarize by MAPE
cgplot <-
  st_drop_geometry(bothRegressions) %>%
  group_by(Regression, npa) %>%
  summarize(mean.MAPE = (mean(Price.APE, na.rm = T))*100) %>%
  ungroup() %>% 
  left_join(Charlotte.nhoods) %>%
  st_sf()
#plot the graph
ggplot() + 
      geom_sf(data=st_union(Charlotte.nhoods), fill="gray", color="black")+
      geom_sf(data=cgplot, aes(fill = q5(mean.MAPE))) +
      scale_fill_manual(values = Blue,
                        labels=qBr(cgplot, "mean.MAPE"), 
                        name = "MAPE")+
      geom_sf(data = bothRegressions, colour = "purple", size = .05, alpha = 0.15) +
      facet_wrap(~Regression) +
      labs(title = "MAPE by Neighborhood", subtitle="Mecklenburg County. 1 dot = 1 observation") +
      mapTheme()

```

# Evaluating our Model

## Accuracy

Our model currently predicts with a ~20% average error. Considering that a down payment for a home is typically 20% of a home's value, under or over estimating by that amount would not be acceptable for nearly any consumer. Our model would need serious revision to be considered competitive with a Zestimate or any similar consumer-facing tool.

That said, our model is quite effective at predicting large patterns in the market, and could be a useful tool for large-scale development and urban planning. We've wrangled the specific points that work in homes favor: livable space, resilient exterior materials, and proximity to schools. Fireplace types and specific land use codes, though rarely not "single-family homes" were surprisingly useful categorical variables. We thought access to groceries and shops would be a greater factor in increasing a homes' price, but some Charlotte residents may prefer distance from commercial activity.

Our model could be improved by being more specific with the categorical variables, utilizing land use code, house style, air conditioning, and others with more targeted factor engineering. This may increase our understanding about how strongly those variables affect the prediction and whether our assumptions are correct about one or two elements being the key indicators within the category.

## Generalizability

After accounting for neighborhood differences through Charlotte's NPA boundaries, our model performed thousands of dollars better than before. This change convinced us that it's more generalizable and can work across broader areas of different incomes.

There are still some points to improve to make the maps work better across the different areas. Our scatterplot of predicted and observed prices made it evident that our model is severely under-estimating a group of about 10 to 20 neighborhoods. Further analysis could reveal whether those are spatially clustered, dispersed, or not, but it's a signal that there may be a predictor that we're not accounting for.

Looking at the MAPE maps just above, there appears to be a remaining collection of higher errors in a semi-circle outside the downtown. Other transportation maps reveal these NPA's are close to highways I-77 and I-85. Though we considered proximity to services, we did not use transportation as a metric for this model, which may have decreased its generalizability across areas with different commute times or access to major arterial roads.

# Conclusion

We would recommend our model to Zillow after further modifications. Although the model is already quite effective for prediction, we see several potential improvements to be made to make it even more stable and powerful. As we mentioned in the introduction, we
have consulted a local family who are currently into buying house for some insights, and here is their answer:

<blockquote>
"We are actually going to start looking into buying a house soon, so
we\'ve talked recently about these factors. Some of our thoughts have
been:

-   Location seems to be top priority, being in a neighborhood close to
    uptown/SouthEnd (a popular area) is prime real estate

-   Schools would be another consideration (Myers Park and Dilworth are
    sought after)

-   I think more about safety (whether I can go out for a walk/run and
    feel safe)

-   Age of house is not much of a determinant for us, but we have a lot
    of friends that have bought/are buying and want something new, and
    in some of the sought after areas, people have been buying based on
    the land, then putting up new houses

-   We also have a dog, as many people do in Charlotte (very
    dog-friendly city), so finding a place with a yard (preferred
    fenced)

-   Being walking distance to things is also an important factor (we
    loved our apartment we use to live in because we could walk to
    Target and Trader Joe\'s, but needed more space)

    We live in an area called South Park, next to the mall here in
    Charlotte, and love it because one road can take us directly into
    the city, so we don\'t normally take the highway to get anywhere,
    which is really nice."
</blockquote>

Although we did take location (neighborhood), schools, safety, year
built, parcel area, and convenient stores into consideration, due to
limited access to free open data, we were not able to engineer factors
that better encapsulate those needs. Hence, some improvements may be:

-   neighborhood as conventionally known instead of administrative
    division of NPA (neighborhood profile area) would be helpful --
    since buyers would look for homes in "South Park" instead of homes
    in "NPA 44"

-   data on schools can be more refined by categorizing primary, middle,
    and high schools, since people don't need 5 primary schools near
    their homes

-   walkability - it would be helpful to have an indicator of
    walkability, not only in terms of distance but also infrastructure
    like safe pedestrian walkways and crossings with traffic lights

-   more indicators related to pet-friendly infrastructure may be
    helpful, too.




![Photo by Zac Gudakov on Unsplash](https://images.unsplash.com/photo-1628983065463-d59659ef4f18?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1742&q=80)

```{r Prediction CSV, warning = FALSE, message = FALSE, include = FALSE}

Charlotte.Prediction <-
  st_drop_geometry(Charlotte.Challenge)%>%
  mutate(prediction = predict(reg.nhood, Charlotte.Challenge))%>%
  dplyr::select(musaID, prediction)
  

write.csv(Charlotte.Prediction,"RegressToImpress.csv", row.names = FALSE)

```
